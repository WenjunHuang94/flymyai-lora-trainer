import os
import json
import uuid
import pandas as pd
from PIL import Image
from io import BytesIO
from multiprocessing import Pool, cpu_count
import glob
###  æŠŠimagenet_1kæŒ‰ç…§ç±»åˆ«æ ‡ç­¾åˆ†ç±»å¹¶ä¿å­˜ä¸ºjpg
PARQUET_DIR = "/storage/v-jinpewang/lab_folder/weiming/datasets/imagenet_1k/data"
# è‡ªåŠ¨æ”¶é›†è¯¥ç›®å½•ä¸‹æ‰€æœ‰ parquet æ–‡ä»¶è·¯å¾„
PARQUET_FILES = sorted(glob.glob(os.path.join(PARQUET_DIR, "*.parquet")))

OUTPUT_ROOT = "/storage/v-jinpewang/lab_folder/weiming/datasets/imagenet_1k/temp_storage/3"
TEMP_JSON_DIR = os.path.join(OUTPUT_ROOT, "temp_jsons")
FINAL_JSON_PATH = os.path.join(OUTPUT_ROOT, "image_label_map.json")
RESIZE_SIZE = 256

DEBUG_MODE = False
DEBUG_NUM = 10  # æ¯ä¸ª parquet ä»…å–å‰ N å¼ 

NUM_PROCESSES = max(len(PARQUET_FILES), cpu_count()-5)


def process_parquet(parquet_path):
    """å¤„ç†å•ä¸ª parquet æ–‡ä»¶"""
    print(f"ğŸ“¦ å¼€å§‹å¤„ç†: {os.path.basename(parquet_path)}")
    df = pd.read_parquet(parquet_path)

    if DEBUG_MODE:
        df = df.head(DEBUG_NUM)
        print(f"âš™ï¸ è°ƒè¯•æ¨¡å¼å¯ç”¨ï¼Œä»…å¤„ç†å‰ {DEBUG_NUM} å¼ å›¾ç‰‡")

    records = []

    for i, row in df.iterrows():
        label = str(row["label"])
        img_bytes = row["image"]["bytes"]

        # åˆ›å»ºæ ‡ç­¾ç›®å½•
        label_dir = os.path.join(OUTPUT_ROOT, label)
        os.makedirs(label_dir, exist_ok=True)

        # ä¿å­˜å›¾ç‰‡
        img_name = f"{uuid.uuid4()}.jpg"
        img_path = os.path.join(label_dir, img_name)

        try:
            img = Image.open(BytesIO(img_bytes)).convert("RGB")
            img = img.resize((RESIZE_SIZE, RESIZE_SIZE), Image.BICUBIC)
            img.save(img_path, format="JPEG")
        except Exception as e:
            print(f"[WARN] {parquet_path} ç¬¬ {i} å¼ ä¿å­˜å¤±è´¥: {e}")
            continue

        records.append({
            "image_path": img_path,
            "label": label
        })

        if i % 100 == 0:
            print(f"{os.path.basename(parquet_path)} å·²å¤„ç† {i}/{len(df)} å¼ å›¾ç‰‡")

    # å†™å…¥ä¸´æ—¶ JSON æ–‡ä»¶
    os.makedirs(TEMP_JSON_DIR, exist_ok=True)
    temp_json_path = os.path.join(TEMP_JSON_DIR, f"{os.path.basename(parquet_path)}.json")
    with open(temp_json_path, "w", encoding="utf-8") as f:
        json.dump(records, f, ensure_ascii=False, indent=4)

    print(f"âœ… å®Œæˆ {parquet_path}ï¼Œå…±ä¿å­˜ {len(records)} å¼ å›¾ç‰‡")
    return temp_json_path


if __name__ == "__main__":
    os.makedirs(OUTPUT_ROOT, exist_ok=True)
    os.makedirs(TEMP_JSON_DIR, exist_ok=True)

    print(f"ğŸš€ å¯åŠ¨å¤šè¿›ç¨‹ï¼Œè¿›ç¨‹æ•°: {NUM_PROCESSES}")

    with Pool(processes=NUM_PROCESSES) as pool:
        temp_json_files = pool.map(process_parquet, PARQUET_FILES)

    # === åˆå¹¶æ‰€æœ‰ JSON ===
    print("\nğŸ§© æ­£åœ¨åˆå¹¶æ‰€æœ‰ä¸´æ—¶ JSON...")
    merged_records = []
    for path in temp_json_files:
        with open(path, "r", encoding="utf-8") as f:
            merged_records.extend(json.load(f))

    with open(FINAL_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(merged_records, f, ensure_ascii=False, indent=4)

    print(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼å…±ä¿å­˜ {len(merged_records)} å¼ å›¾ç‰‡")
    print(f"ğŸ‘‰ æœ€ç»ˆ JSON æ–‡ä»¶è·¯å¾„: {FINAL_JSON_PATH}")
