import time
from diffusers import DiffusionPipeline
import torch

# è®°å½•æ€»å¼€å§‹æ—¶é—´
total_start_time = time.time()

# --- 1. åŠ è½½åŸºç¡€ Qwen-Image æ¨¡å‹ ---
print("=" * 50)
print("æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹ Qwen/Qwen-Image ...")
model_load_start = time.time()

model_name = "Qwen/Qwen-Image"
if torch.cuda.is_available():
    torch_dtype = torch.bfloat16
    device = "cuda"
else:
    torch_dtype = torch.float32
    device = "cpu"

pipe = DiffusionPipeline.from_pretrained(model_name, torch_dtype=torch_dtype)  # TODO: ä¹Ÿå¯ä»¥ç›´æ¥è°ƒç”¨QwenImagePipeline
pipe = pipe.to(device)

# =======================================================
# ğŸ”½ åœ¨è¿™é‡ŒåŠ å…¥æ‰“å°ä»£ç  ğŸ”½
# =======================================================

print("\n\n" + "=" * 80 + "\n")
print("=" * 25 + " 1. VAE ç»“æ„ (pipe.vae) " + "=" * 25)
# pipe.vae å°±æ˜¯è„šæœ¬1ä¸­çš„ AutoencoderKLQwenImage å®ä¾‹
print(pipe.vae)

print("\n\n" + "=" * 80 + "\n")
print("=" * 20 + " 2. DiT ç»“æ„ (pipe.transformer) " + "=" * 20)
# pipe.transformer å°±æ˜¯è„šæœ¬1ä¸­çš„ QwenImageTransformer2DModel å®ä¾‹
print(pipe.transformer)

print("\n\n" + "=" * 80 + "\n")
print("=" * 20 + " 3. æ–‡æœ¬ç¼–ç å™¨ ç»“æ„ (pipe.text_encoder) " + "=" * 20)
# pipe.text_encoder å°±æ˜¯è„šæœ¬1ä¸­çš„ Qwen2_5_VLForConditionalGeneration å®ä¾‹
print(pipe.text_encoder)
print("\n\n" + "=" * 80 + "\n")

# =======================================================
# ğŸ”¼ æ‰“å°ä»£ç ç»“æŸ ğŸ”¼
# =======================================================

model_load_time = time.time() - model_load_start
print(f"åŸºç¡€æ¨¡å‹åŠ è½½å®Œæ¯•ã€‚è€—æ—¶: {model_load_time:.2f} ç§’")
print("=" * 50)

# --- 2. åŠ è½½æ‚¨è®­ç»ƒå¥½çš„ LoRA æƒé‡ ---
lora_load_start = time.time()
# ã€ä¿®æ”¹ç‚¹ 1ã€‘: è·¯å¾„å·²æ›´æ–°ä¸ºæ‚¨è‡ªå·±çš„ checkpoint è·¯å¾„
lora_file_path = "/home/disk2/hwj/flymyai-lora-trainer/output/checkpoint-250/pytorch_lora_weights.safetensors"
# lora_file_path = "/home/disk2/hwj/flymyai-lora-trainer/qwen-image-realism-lora/flymy_realism.safetensors"

try:
    print(f"æ­£åœ¨åŠ è½½æ‚¨è®­ç»ƒçš„ LoRA æ–‡ä»¶: {lora_file_path} ...")
    # ã€ä¿®æ”¹ç‚¹ 2ã€‘: å–æ¶ˆäº†æ³¨é‡Šï¼Œå¹¶æ›´æ”¹äº† adapter_name
    pipe.load_lora_weights(lora_file_path, adapter_name="hwj")  # <--- å·²å–æ¶ˆæ³¨é‡Š
    print("è®¾ç½® adapter_name ä¸º 'hwj'")

    lora_load_time = time.time() - lora_load_start
    print(f"LoRA åŠ è½½æˆåŠŸï¼è€—æ—¶: {lora_load_time:.2f} ç§’")
except Exception as e:
    print(f"åŠ è½½ LoRA å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿ lora_file_path è·¯å¾„æ­£ç¡®ã€‚")
    exit()

print("=" * 50)

# --- 3. å‡†å¤‡æç¤ºè¯ (Prompt) ---
# ã€ä¿®æ”¹ç‚¹ 3ã€‘: ä½¿ç”¨æ‚¨çš„è§¦å‘è¯å’Œç¤ºä¾‹æç¤º
prompt = '''ohwhwj man, wearing a red helmet and orange life jacket, smiling with water droplets on his face.'''

positive_magic = ", Ultra HD, 4K, cinematic composition."
negative_prompt = " "

# --- 4. ç”Ÿæˆå›¾åƒ ---
print("æ­£åœ¨ç”Ÿæˆå›¾åƒ (ä½¿ç”¨ 'ohwhwj man' LoRA)...")
generation_start = time.time()

image = pipe(
    prompt=prompt + positive_magic,
    negative_prompt=negative_prompt,
    width=512,
    height=512,
    num_inference_steps=50,
    true_cfg_scale=5,
    generator=torch.Generator(device="cuda").manual_seed(42)  # æ‚¨å¯ä»¥æ¢ä¸ª seed çœ‹çœ‹æ–°æ•ˆæœ, æ¯”å¦‚ 12345
).images[0]

generation_time = time.time() - generation_start
print(f"å›¾åƒç”Ÿæˆå®Œæˆï¼è€—æ—¶: {generation_time:.2f} ç§’")
print("=" * 50)

# --- 5. ä¿å­˜å›¾åƒ ---
save_start = time.time()
# ã€ä¿®æ”¹ç‚¹ 4ã€‘: æ›´æ”¹äº†è¾“å‡ºæ–‡ä»¶å
output_filename = "output_hwj_checkpoint1000-9.png"
image.save(output_filename)
save_time = time.time() - save_start

print(f"å›¾åƒå·²æˆåŠŸä¿å­˜ä¸º: {output_filename}")
print(f"ä¿å­˜è€—æ—¶: {save_time:.2f} ç§’")

# --- 6. ç»Ÿè®¡æ€»è€—æ—¶ ---
total_time = time.time() - total_start_time
print("=" * 50)
print("=== æ—¶é—´ç»Ÿè®¡æ±‡æ€» ===")
print(f"åŸºç¡€æ¨¡å‹åŠ è½½: {model_load_time:.2f} ç§’")
print(f"LoRA æƒé‡åŠ è½½: {lora_load_time:.2f} ç§’")
print(f"å›¾åƒç”Ÿæˆ: {generation_time:.2f} ç§’")
print(f"å›¾åƒä¿å­˜: {save_time:.2f} ç§’")
print("-" * 30)
print(f"æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time / 60:.2f} åˆ†é’Ÿ)")
print("=" * 50)